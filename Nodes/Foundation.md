### 名词翻译
| 中文 | 英文 |
| :----: | :----: |
| 超参数 | Hyperparameter |
| 标准化 | Standardization |
| 归一化 | Normalization |
| 归纳偏置 | Inductive Bias |
| 决策边界 | Decision Boundary |

### 贝叶斯定理
* [怎样用非数学语言讲解贝叶斯定理(Bayes's Theorem)](https://www.zhihu.com/question/19725590/answer/17477686)

### 超参数 Hyperparameter
* [超参数](https://baike.baidu.com/item/%E8%B6%85%E5%8F%82%E6%95%B0/3101858)

### 标准化 Standardization 和 归一化 Normalization
* [标准化和归一化什么区别?](https://www.zhihu.com/question/20467170)
* [为什么要做特征归一化/标准化?](https://www.cnblogs.com/shine-lee/p/11779514.html)
* [详解深度学习中的Normalization -- BN/LN/WN](https://zhuanlan.zhihu.com/p/33173246)
* [机器学习——标准化/归一化的目的和作用](https://blog.csdn.net/zenghaitao0128/article/details/78361038)
* 所有涉及到距离的都需要标准化
    * KNN, PCA, SVM, Deep Learning
* 不涉及距离的则不需要标准化
    * 树模型, Linear Regression, Logistic Regression

### 归纳偏置 Inductive Bias
* [如何理解Inductive bias？](https://www.zhihu.com/question/264264203)
* [深度学习的归纳偏置是什么？](https://www.zhihu.com/question/41404496/answer/627673667)
* [[Machine Learning] 归纳偏置](https://zhuanlan.zhihu.com/p/52103577)
* [理论知识学习32】归纳偏差与选择性偏差（概念作用以及举例说明）](https://blog.csdn.net/qq_41554005/article/details/115897128)
* [小结某些机器学习算法归纳偏置](https://blog.csdn.net/u012750702/article/details/54381632)
* [The Inductive Biases of Various Machine Learning Algorithms](http://www.lauradhamilton.com/inductive-biases-various-machine-learning-algorithms)

### 决策边界 Decision Boundary
* [机器学习算法的决策边界（decision boundary）](https://blog.csdn.net/jodie123456/article/details/88870616)

### 权重/参数初始化 Weight Initialization
* [啃一啃神经网络 -- 权重初始化](https://zhuanlan.zhihu.com/p/102708578)
* [深度学习中神经网络的几种权重初始化方法](https://blog.csdn.net/u012328159/article/details/80025785)
* [为什么神经网络参数不能全部初始化为全0?(关注评论)](https://blog.csdn.net/qq_15505637/article/details/79362970)
* [神经网络中的权重初始化一览：从基础到Kaiming](https://zhuanlan.zhihu.com/p/62850258)

### 噪声输入 Noisy Input 和 噪声目标 Noisy Target
* [机器学习中的噪音(机器学习基石)](https://blog.csdn.net/qq_34993631/article/details/79234483)
* [Error and Noise](http://www.stat.ucdavis.edu/~chohsieh/teaching/ECS171_Winter2018/lecture5.pdf)
* [Noise and Error - Noise and Probabilistic Target](https://www.bilibili.com/video/BV1Cx411i7op?p=30)
* [Stochastic and Deterministic Noise](http://www.cs.rpi.edu/~magdon/courses/LFD-Slides/SlidesLect11.pdf)

### 目标函数
* [机器学习中的目标函数, 损失函数, 代价函数有什么区别?](https://www.zhihu.com/question/52398145)

### 概率函数P(x), 概率分布函数F(x), 概率密度函数f(x)
* [概率函数P(x), 概率分布函数F(x), 概率密度函数f(x)](https://www.jianshu.com/p/0cfc3204af77)
* [理解概率密度函数](https://sigai.blog.csdn.net/article/details/83586458)

### 似然函数
* [如何理解似然函数?](https://www.zhihu.com/question/54082000)

### 极大似然估计
* [详解最大似然估计(MLE), 最大后验概率估计(MAP), 以及贝叶斯公式的理解](https://blog.csdn.net/u011508640/article/details/72815981)
* [一文搞懂极大似然估计](https://zhuanlan.zhihu.com/p/26614750)